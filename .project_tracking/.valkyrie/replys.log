Here’s a single paste-ready block JR can drop in to keep momentum:

# CNS – Post-Green Next Steps (JR Instructions)

## 0) Merge hygiene (do now)
- Open PR titled: `test: per-test ephemeral DB isolation + green suite`
- Include summary from your status note (exact bullets).
- Check boxes:
  - [x] All tests pass locally
  - [x] Demo seeds stable
  - [x] No global DELETEs remain
  - [x] `tests/admin_db.py` uses psycopg v3
  - [x] `tests/conftest.py` autouse per-test clone works on fresh Docker

---

## 1) CI polish with perf artifacts (today)
Add `.github/workflows/ci.yml` (or patch it) to:
- Run Postgres on **5433** as a service
- Export DSNs with port 5433
- Always run `scripts/perf_smoke.py` and upload **`perf_last_explain.json`** + **`perf_summary.md`**

```yaml
name: ci
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: cns
          POSTGRES_PASSWORD: cns
          POSTGRES_DB: cns
        ports: ["5433:5432"]
        options: >-
          --health-cmd="pg_isready -U cns -d cns"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=12

    env:
      CNS_DSN: "dbname=cns user=cns password=cns host=127.0.0.1 port=5433"
      CNS_ADMIN_DSN: "dbname=postgres user=cns password=cns host=127.0.0.1 port=5433"
      PYTHONUNBUFFERED: "1"

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run tests
        run: |
          pytest -q

      - name: Perf smoke (always run)
        if: always()
        run: |
          python scripts/perf_smoke.py || true

      - name: Upload perf artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: perf-artifacts
          path: |
            perf_last_explain.json
            perf_summary.md


Create scripts/perf_smoke.py:

#!/usr/bin/env python3
import os, json, time
from datetime import datetime
from cns_py.cql.executor import run_cql
from cns_py.telemetry import Telemetry  # add simple helper; see note below

QUERIES = [
    ("asof_2024", "MATCH (f:Framework{name:'FrameworkX'})-[r:supports_tls]->(c:Concept) ASOF '2024-12-31' RETURN f,c,r EXPLAIN PROVENANCE;"),
    ("asof_2025", "MATCH (f:Framework{name:'FrameworkX'})-[r:supports_tls]->(c:Concept) ASOF '2025-09-30' RETURN f,c,r EXPLAIN PROVENANCE;"),
]

def main():
    summary = {
        "started_at": datetime.utcnow().isoformat()+"Z",
        "dsn": os.getenv("CNS_DSN"),
        "results": []
    }
    for name, q in QUERIES:
        t0 = time.perf_counter()
        out = run_cql(q)  # must return dict with 'rows', 'explain', 'citations'
        t1 = time.perf_counter()
        explain = out.get("explain", {})
        rows = out.get("rows", [])
        citations_ok = all(len(r.get("citations", [])) >= 1 for r in rows)
        summary["results"].append({
            "name": name,
            "elapsed_ms": round((t1 - t0)*1000, 1),
            "row_count": len(rows),
            "citations_ok": citations_ok,
            "explain": {
                "ann_ms": explain.get("ann_ms"),
                "mask_ms": explain.get("mask_ms"),
                "traverse_ms": explain.get("traverse_ms"),
                "rules_ms": explain.get("rules_ms"),
                "belief_before_after": explain.get("belief_terms"),  # {edge_id: {before, after, terms:{evidence,rep,recency,contradictions}}}
            },
        })

    with open("perf_last_explain.json", "w") as f:
        json.dump(summary, f, indent=2)

    # quick human summary
    lines = ["# Perf Smoke Summary"]
    for r in summary["results"]:
        lines.append(f"- **{r['name']}**: {r['elapsed_ms']} ms · rows={r['row_count']} · citations_ok={r['citations_ok']} · ann/mask/trav/rules = {r['explain']['ann_ms']}/{r['explain']['mask_ms']}/{r['explain']['traverse_ms']}/{r['explain']['rules_ms']} ms")
    with open("perf_summary.md", "w") as f:
        f.write("\n".join(lines))

if __name__ == "__main__":
    main()


Note: Implement a minimal cns_py/telemetry.py to collect stage timings inside the executor; or embed timings directly into the executor’s explain dict. The smoke script just reads them.

2.5) Guardrails (add to executor now)

Executor must populate in EXPLAIN:

ann_ms, mask_ms, traverse_ms, rules_ms

belief_terms → {"edge_id": {"before": x, "after": y, "terms": {"evidence":a,"source_rep":b,"recency":c,"contradictions":d}}}

If no citations, drop the claim from results (contract).

3) Config flags (today)

Create cns_py/config.py:

import os

ASOF_END_INCLUSIVE = os.getenv("CNS_ASOF_END_INCLUSIVE", "0") == "1"
def temporal_predicate():
    # Returns SQL fragment for end boundary
    return "(valid_to IS NULL OR valid_to >= %(ts_to)s)" if ASOF_END_INCLUSIVE else "(valid_to IS NULL OR valid_to > %(ts_to)s)"


Use temporal_predicate() in temporal SQL builders. Add a note in README that boundary semantics are configurable; golden tests pin the default.

4) Planner/perf (next 1–2 days)

Cache ANN shortlist results for identical query text within a test (LRU of 64).

Use bitmap masks for temporal filtering (e.g., fetch candidate edge ids then apply Python-side bitarray/roaringbitmap if available; SQL acceptable for now, but log mask_ms).

Ensure traverse_from uses parameterized IN (%s, %s, ...) properly; for multiple predicates use = ANY(%s) (you already did).

Add a P95 tracker over last N runs in dev (simple deque); print to perf_summary.md.

5) CI pass criteria (this PR)

✅ Tests 100% green on Actions with Postgres service (5433).

✅ perf_last_explain.json and perf_summary.md uploaded as artifacts.

✅ Each smoke query returns ≥1 row with citations.

✅ EXPLAIN contains stage timings and belief term breakdowns.

6) After merge (follow-on PRs to queue)

Add a badge (“perf smoke: last run < 300 ms average”) by reading the artifact in a tiny GH Page or generate a summary comment via a workflow.

Introduce WHY_NOT operator skeleton (returns empty list for now, with explain.gap_search_ms).

Start IB Explorer scaffolding (Three.js baseline; static mock data), separate PR.